---
title: "Project"
author: "Irtza Shahan"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: sentence
---

## Loading the dataset

```{r}
library(readxl)
library(dplyr)
library(ggplot2)

Movies <- read_excel("Movie.xls")
View(Movies)
attach(Movies)
str(Movies)
```

## Conversion of ordinal variables to factor

```{r}

Movies$Rating <- as.factor(Movies$Rating)
Movies$Sequel <- as.factor(Movies$Sequel)
Movies$Genre <- as.factor(Movies$Genre)

str(Movies)
summary(Movies)

```

## Checking for missing values

```{r}
colSums(is.na(Movies))
```

## Creating boxplots

```{r}

#Defininge numeric columns for boxplots and other uses like scatter plot and outliers detection
numeric_cols <- c("Budget", "Opening", "USRevenue", "Theaters","IntRevenue", "WorldRevenue","Ratings","Review", "Minutes")

# Create boxplots 
par(mfrow = c(2, 2), mar = c(5, 6, 4, 2))  # Increase margin for labels

for (col in numeric_cols) {
  boxplot(Movies[[col]],
          main = paste( col),
          col = "lightblue",
          border = "darkblue",
          ylab = col,
          cex.main = 1.5,     # Increase title size
          cex.axis = 1.2,     # Increase axis text
          cex.lab = 1.2)      # Increase axis label size
}


```

Budget variable has no apparent outliers.
Opening has positive outliers meaning some movies had unusually high opening earnings.
USRevenue has positive outliers showing a few movies had very high U.S. revenue compared to the rest.
Theaters has no apparent outliers.
IntRevenue has positive outliers, reflecting high international earnings for a few blockbuster movies.
WorldRevenue has positive outliers meaning a few movies had much higher total global revenue.
Ratings has negative outliers, meaning some movies received significantly low ratings.
Review has positive outliers showing certain movies received many more reviews than average.

## Counting the outliers

```{r}

# Count outliers using IQR method
outlier_counts <- Movies %>%
  select(all_of(numeric_cols)) %>%
  summarise(across(everything(), ~ sum(
    . < quantile(., 0.25, na.rm = TRUE) - 1.5 * IQR(., na.rm = TRUE) |
    . > quantile(., 0.75, na.rm = TRUE) + 1.5 * IQR(., na.rm = TRUE)
  )))

# Print result
print(outlier_counts)

```

Budget variable has no apparent outliers.
Opening has 2 positive outliers.
USRevenue has 2 positive outliers.
Theaters has no apparent outliers.
IntRevenue has 1 positive outlier.
WorldRevenue has 2 positive outliers.
Ratings has 2 negative outliers.
Review has 1 negative outlier.
Minutes has no apparent outliers.

## Creating QQ plots for checking normality

```{r}

# Plotting layout: 3 rows x 3 columns
par(mfrow = c(2, 2), mar = c(5, 5, 4, 2))  

# Generate Q-Q plots
for (col in numeric_cols) {
  qqnorm(Movies[[col]],
         main = paste(col),
         col = "darkblue",
         pch = 19,
         cex.main = 1.5,  
         cex.lab = 1.3,   
         cex.axis = 1.2)  
  
  qqline(Movies[[col]], col = "red", lwd = 2)
}


```

Budget – Right-skewed; not normally distributed due to high-budget outliers.

Opening – Right-skewed; deviates from normality with a few unusually high openings.

USRevenue – Strong right skew; distribution is not normal.

Theaters – Slight right skew; approximately normal with mild deviation.

IntRevenue – Right-skewed; not normally distributed due to blockbuster outliers.

WorldRevenue – Right-skewed; deviates from normality with heavy upper tail.

Ratings – Nearly normal; minor deviation at the tails.

Review – Slight right skew; close to symmetric but not perfectly normal.

Minutes – Approximately normal; points mostly follow the normal line.

## Test for checking the normality

$h_0:$ variable is normally distributed $h_a:$ variable is not normally dirstributed

```{r}
# Apply Shapiro-Wilk test for all numeric variables
shapiro_results <- sapply(Movies[numeric_cols], function(x) shapiro.test(x)$p.value)
print(round(shapiro_results, 4))

```

Normally Distributed: Theaters, Review

Not Normally Distributed: Budget, Opening, USRevenue, IntRevenue, WorldRevenue, Ratings, Minutes

## Summary statistics

```{r}
#Summary Statistics of Numerical Columns 
summary(Movies[numeric_cols])
```

```{r}

# Create the scatterplot matrix
pairs(Movies[, numeric_cols],
      main = "Scatterplot Matrix of Numeric Variables",
      pch = 19,
      cex = 0.8,
      col = rgb(0, 0, 1, 0.3),
      lower.panel = NULL,
      labels = colnames(Movies[, numeric_cols]),
      gap = 0.5)

windows(width = 12, height = 12)
options(repr.plot.width = 20, repr.plot.height = 20)

```

```{r}
library(ggcorrplot)
cor_matrix <- cor(Movies[numeric_cols], use = "pairwise.complete.obs")

ggcorrplot(cor_matrix,
           method = "square",         
           type = "lower",            
           lab = TRUE,                
           lab_size = 3,              
           colors = c("red", "white", "blue"),
           title = "Correlation Matrix Heatmap",
           ggtheme = theme_minimal())

options(repr.plot.width = 20, repr.plot.height = 20)

```

#checking multicollinearity
Since IntRevenue and WorldRevenue is part of each other and USRevenue our dependent variable, we will not be considering them
```{r}

library(car)
# full first order model
Model1 =  lm(USRevenue~Budget+Opening+Theaters+Ratings+Review+Minutes)
#checking vif
vif_values = vif(Model1)
high_vif = vif_values[vif_values > 5]
data.frame(VIF = vif(Model1))

```

So none of the numerical variables have multicollinearity over 5
## plotting corr and Scatterplot matrix 
```{r}
library(PerformanceAnalytics)
chart.Correlation(Movies[, c('Budget','Theaters',"Ratings",'Review','Minutes')],main = "Scatterplot Matrix of Variables in model1")
```

```{r}
null_model = lm(USRevenue~1)
#after adding categorical variables
model2 =  lm(USRevenue~Genre+Rating+Sequel+Budget+Opening+Theaters+Ratings+Review+Minutes,data=Movies)
summary(model2)

```


Since Sequel and Rating are categorical variables and as we can see above they are not significant as well so we are going to drop them here before stepwise regression i.e not include them in pool for stepwise regression.

```{r}
model3 = lm(USRevenue~Genre+Budget+Opening+Theaters+Ratings+Review+Minutes,data=Movies)
summary(model3)
```

# stepwise regression
```{r}
best_step = step(model3,direction="both")
best_step
```

#### Significant variables: Opening + Genre + Ratings + Theaters


# Forward selection
```{r}


forwardAIC <- step(null_model,scope=list(lower=~1, 
                                   upper=~Genre+Budget+Opening+Theaters+Ratings+Review+Minutes),
                   direction="forward", data=Movies)
forwardAIC
```
#### Significant variables(best step frwrd): Opening + Genre + Ratings + Theaters


# Backward Elimination
```{r}
best_step_bckwrd = step(model3,direction="backward")
best_step_bckwrd
```
#### Significant variables(best step bckwrd): Opening + Genre + Ratings + Theaters


## Best Subset
```{r}

library(leaps)
Model=regsubsets(USRevenue~Genre+Budget+Opening+Theaters+Ratings+Review+Minutes, data =Movies, nvmax = 7)
summary(Model)

SUM=summary(Model)
names(SUM)
Rsq=SUM$rsq
CP=SUM$cp
AdRsq=SUM$adjr2
BIC=SUM$bic
RSS=SUM$rss
#Calculation of AIC
n <- length(USRevenue)
p <- apply(SUM$which, 1, sum)
AIC<- SUM$bic - log(n) * p + 2 * p
#number of independent variables in the models
I=p-1
I
MSE1=RSS/(n-I-1)
MSE1

###Plot
par(mfrow=c(2,2))
plot(p,Rsq,xlab="Subset Size",ylab="Adjusted R-squared", ylim=c(0.92,.97),pch=19, col="blue")
plot(p,Rsq,xlab="Subset Size",ylab="R-squared", ylim=c(0.92,.97), pch=19, col="blue")
plot(p,CP,xlab="Subset Size",ylab="CP", ylim=c(0, 25), pch=19, col="blue")
#lines(y=p+1,x=p, col="red")
```

```{r}
m1 <- lm(USRevenue ~ Genre, data = Movies)
s1 <- summary(m1)

m2 <- lm(USRevenue ~ Genre + Budget, data = Movies)
s2 <- summary(m2)

m3 <- lm(USRevenue ~ Genre + Budget + Opening, data = Movies)
s3 <- summary(m3)

m4 <- lm(USRevenue ~ Genre + Budget + Opening + Theaters, data = Movies)
s4 <- summary(m4)

m5 <- lm(USRevenue ~  Genre + Budget + Opening + Theaters + Ratings, data = Movies)
s5 <- summary(m5)

m6 <- lm(USRevenue ~  Genre + Budget + Opening + Theaters + Ratings + Review, data = Movies)
s6 <- summary(m6)

m7 <- lm(USRevenue ~  Genre + Budget + Opening + Theaters + Ratings + Review + Minutes, data = Movies)
s7 <- summary(m7)

#  PRESS stats without the library qpcR because it has bugs and not working correctly for us

press_stat <- function(model) {
  r <- residuals(model)
  h <- hatvalues(model)
  sum((r / (1 - h))^2)
}

models   <- list(m1, m2, m3, m4, m5, m6, m7)
PRESS    <- sapply(models, press_stat)
MSE      <- sapply(models, function(m) summary(m)$sigma^2)


all_vars <- attr(terms(m5), "term.labels")
Xflags   <- t(sapply(models, function(mod) {
  as.integer(all_vars %in% attr(terms(mod), "term.labels"))
}))
colnames(Xflags) <- all_vars 
out_tbl  <- cbind(Xflags,round(cbind(Rsq, AdRsq, CP, BIC, RSS,AIC, PRESS, MSE), 7))
out_tbl
```

**interpretation**
$Adj R^{2}$ rises through m5 and peaks there (0.956), but the gain from m4 m5 is tiny.

PRESS / MSE and AIC reach their minima at m5, yet the improvements over m4 are slight. Same goes for BIC and AIC
CP is closest to P in m3, while m4 and m5 show much larger gaps, suggesting diminishing returns beyond three predictors.
The increase in $R^2$ and the decrease in RSS in m4 compared to m3 are negligible and do not justify the added complexity (one more predictor).
RSS steadily falls with added variables, but the m4 to m5 reduction ( 480 ) is small relative to earlier drops.


so according to all observations model m4 (Genre + Budget + Opening + Theaters) is the statistically preferred choice; m5’s extra variable adds negligible benefit relative to its additional complexity.

so according to stepwise regresssion (forward and backword both) and then also according to above observations we can decide that we should only keep Genre + Budget + Opening + Theaters in our model and drop the rest of the variables from our model


```{r}
# chosen model after stepwise regression, best subset and press stats observations
model4 = m4  # (lm(Genre + Budget + Opening + Theaters))
summary(model4)
```

```{r}
df_without_log_y <- data.frame(USRevenue, Budget   = Movies$Budget,   Opening  = Movies$Opening,   Theaters = Movies$Theaters )
chart.Correlation(df_without_log_y,  main = "Scatterplot Matrix of remaining numerical Variables after stepwise regression")

df_with_log_y <- data.frame(log(Movies$USRevenue), Budget   = Movies$Budget,   Opening  = Movies$Opening,   Theaters = Movies$Theaters )
chart.Correlation(df_with_log_y,  main = "Scatterplot Matrix of remaining numerical Variables after stepwise regression")

```
#### The log-transformed US Revenue is nearly symmetric and produces straighter, more homoscedastic scatter relationships with the predictors, whereas the unlogged values are highly right-skewed and show fan-shaped variance—so the log scale aligns much better with linear-model assumptions.

# transformations
```{r}
trasnformed_model1 = lm(log(USRevenue)~ Genre + Budget + Opening + Theaters, data = Movies)
# Summary of the model
summary(trasnformed_model1)
```

Since after trasnformation we can see most of the predictor power comes from only 2 variables so we will remove the rest of the 2 variables (Genre and Budget) and try with opening and Theaters only


```{r}
trasnformed_model2 = lm(log(USRevenue)~ Opening + Theaters, data = Movies)
summary(trasnformed_model2)
```
Here adj r square is reduced from after log of only y so to fit the model better we will also try with with log of explanatory variables.

```{r}
trasnformed_model3 = lm(log(USRevenue)~ log(Opening) + Theaters, data = Movies)
summary(trasnformed_model3)
new_log_var_df <- data.frame(log(USRevenue), log(Opening), Theaters)
chart.Correlation(new_log_var_df,  main = "Scatterplot Matrix of remaining numerical Variables after stepwise regression")
```
After transformation of only opening we see model fits better but theaters is not significant anymore in model now as it has pvalue of 0.47. But in matrix plot and we don't see any curvature and it is strongly collinear with log(Opening) so using log of theaters is not likely to yield any better results as well so here we will try to remove theaters from our model and check the model with only opening.

```{r}
trasnformed_model4 = lm(log(USRevenue)~ log(Opening), data = Movies)
summary(trasnformed_model4)
```




```{r}
# Adding predicted and observed values (log scale)
Movies$predicted_log_USRevenue <- predict(trasnformed_model4)
Movies$observed_log_USRevenue <- log(Movies$USRevenue)

#making a comparison table
comparison_df= data.frame(
  Opening = log(Movies$Opening),
  Observed = Movies$observed_log_USRevenue,
  Predicted = Movies$predicted_log_USRevenue,
  residuals = Movies$observed_log_USRevenue - Movies$predicted_log_USRevenue
)
comparison_df
#residuals plot
plot(Movies$observed_log_USRevenue, comparison_df$residuals, pch=19, col="blue",
     xlab = "Fitted Values",
     ylab  = "Residuals",
     main= "Residuals vs Fitted")
abline(h=0, lty="dashed", col="red")

```

The residual plot shows that the residuals are randomly scattered around zero, indicating that the linearity assumption holds for the model. There is no clear pattern, though a slight increase in spread suggests mild heteroscedasticity. Overall, the model appears appropriate for the data.



```{r}

# Ploting observed vs. predicted values (log scale)
plot(Movies$observed_log_USRevenue, Movies$predicted_log_USRevenue,
     xlab = "Observed log(USRevenue)",
     ylab = "Predicted log(USRevenue)",
     main = "Observed vs. Predicted Values (Log Scale)",
     col = "blue", pch = 19)
abline(0, 1, col = "red", lwd = 2)

```
This observed vs. predicted plot (on the log scale) shows that most points lie close to the red diagonal line, indicating a strong agreement between actual and predicted values. The alignment suggests that the model fits the data well, with minimal systematic bias. A few deviations are present, but overall the predictions are accurate and consistent.


```{r}
leverage <- round(hatvalues(trasnformed_model4),3)
StanRes <- round(rstandard(trasnformed_model4),3)
residual <- round(trasnformed_model4$residuals,3)
cd <- round(cooks.distance(trasnformed_model4),3)
Rstudent=round(rstudent(trasnformed_model4),5)
A=cbind(log(USRevenue), Opening, Theaters,leverage,residual,StanRes, cd,Rstudent )

B=data.frame(A)

#Cutoff for Detecting Influence with Leverage
n=length(USRevenue)
cutL=2*(4+1)/n

which(B$leverage >cutL)

#Cook’s Distance Cutoff

cutCD=4/(n-4-1)

which(B$cd >cutCD)

A[which(B$cd >cutCD),]

subset(B,B$cd >cutCD&B$leverage >cutL)

plot(cd,main="Cook’s Distance", xlab="Observation", ylab="Cook’s Distance Cutoff
")
abline(h=cutCD, lty="dashed", col="red")

```
Interpretation

- **Cook’s distance**: three cases, observation 1,3,43,44, slightly exceeds the influence cut off $0.10$, indicating they have a measurable but modest impact on the fitted values.
- **Leverage:** the same observations does not exceed the high-leverage threshold of $H_{crit}=0.23$, so it is not an extreme design point.
- **Joint criterion:** no observation meets both $Di>D_{crit}$ and $hii>h_{crit}$ the subset query returned 0 rows.


```{r}

plot(trasnformed_model4)

```
Q–Q Residuals
Most standardized residuals align with the 45 ° reference; the lower tail bends slightly downward and the upper tail (obs 16) rises above the line, indicating mild heavy-tailed behaviour. Overall, the normal-error assumption is acceptable but the extremes deviate modestly.

Scale-Location (standardised residuals vs Fitted)
The loess curve is nearly flat, dipping in the centre and edging up for the largest fitted values, so residual variance is essentially constant with only a slight uptick at high predicted revenue—heteroskedasticity is minor.

Residuals vs Leverage (Cook’s Distance overlay)
Leverage values top out at = 0.14 (well below the high-leverage cut-off 0.23) and every Cook’s distance lies far under 0.5. The few labelled cases (e.g., 1, 30, 44) have moderate leverage but modest residuals, so no observation is influential enough to distort the fitted coefficients.



# F test

**Hypotheses**
$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$

```{r}
summary(trasnformed_model4)
```
**F statistics: 924.8**
**p-value:2.2e-16**
here p value approaches to zero means we will reject the null hypothesis.
We can say, The model is statistically significant.


# t test for predictors

# For log(Openning)
**hypothesis**
$H_0: \beta_1 =0$
$H_a: \beta_1 \neq 0$

**Test statistics:30.411 **
**p-value:  <2e-16** which is approaches to 0, means we reject the null hypothesis. So theaters is a strong significant predictor.


# Anova

trasnformed_model3 <- lm(log(USRevenue) ~ log(Opening) + Theaters, data = Movies)

trasnformed_model4: lm(log(USRevenue) ~ log(Opening), data = Movies)

**hypothesis**
$H_0:  \beta_2 = 0$ The reduced model is addequate.

$h_a: \beta_2 \neq 0$ trasnformed model with theaters fits better

```{r}
library(car)
anova(trasnformed_model4,trasnformed_model3)

```
**F statistic:0.524 **
**p - value: 0.4733 **
**decision: ** Since $P-value > \alpha=  0.05$ we fail to Reject the $h_0$, the reduced model with log of opening fits fine without the theaters, i.e $\beta_2 = 0$


#95% CI 

```{r}

## 95 % CI on the multiplicative scale
ci_log  <- confint(trasnformed_model4, level = 0.95)  # original log-scale limits
ci_exp  <- exp(ci_log)                               # exponentiated limits

ci_exp

```
If Opening is multiplied by about 2.74x, expected U.S. revenue is multiplied by 2.74x to 3.16× (from the 95% CI for log(Opening))
The baseline multiplier (intercept) on the revenue scale is 1.78x to 2.82×

#95% PI

#maximum USrevenue
```{r}
# Find the row with the maximum USRevenue
top_movie <- Movies[which.max(Movies$USRevenue), c("Opening","Theaters")]

output=predict(trasnformed_model4,newdata = top_movie,interval = "prediction",level = 0.95)
output
exp(output)


```
For the highest-grossing movie in the dataset, with a given number of theaters and ratings score, the interaction model predicts a USRevenue of approximately $380.8267  million, with a 95% prediction interval ranging from $253.2314 million to $572.7132 million.


#Minimum USRevenue
```{r}
# Find the row with the minimum USRevenue
least_revenue_movie <- Movies[which.min(Movies$USRevenue), c("Opening","Theaters")]


output1=predict(trasnformed_model4,newdata = least_revenue_movie,interval = "prediction",level = 0.95)
output1
exp(output1)

```

For the lowest-grossing movie in the dataset, with a given number of theaters and ratings score, the reducedmodel5 predicts a USRevenue of approximately $12.16303  million, with a 95% prediction interval ranging from $8.105205  million to $18.25237 million.

